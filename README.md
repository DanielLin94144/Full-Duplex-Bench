# Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities
> Authors: Guan-Ting Lin, Jiachen Lian*, Tingle Li*, Qirui Wang*, Gopala Anumanchipalli, Alexander H. Liu, Hung-yi Lee

## TL;DR
A benchmark to evaluate full-duplex spoken dialogue models on pause handling, backchanneling, turn-taking, and user interruptions.

[![arXiv](https://img.shields.io/badge/arXiv-2409.06666-b31b1b.svg?logo=arXiv)]([https://arxiv.org/abs/2409.06666](https://arxiv.org/abs/2503.04721))
[![code](https://img.shields.io/badge/Github-Code-keygen.svg?logo=github)](https://github.com/DanielLin94144/Full-Duplex-Bench)

<div align="center"><img src="https://github.com/user-attachments/assets/70b6525c-61ee-4c48-a1fb-59dc6dfe85cc" width="85%"/></div>

## Highlight üí°
Spoken dialogue modeling presents unique challenges beyond text-based language modeling, requiring real-time interaction capabilities such as turn-taking, backchanneling, and pause handling. Existing evaluation methods primarily focus on half-duplex processing, leaving the full-duplex capabilities of modern models underexplored. 

Full-Duplex-Bench provides an open and standardized benchmark to assess these interactive behaviors systematically. Our framework leverages automatic metrics for consistent and reproducible evaluations across key dimensions:

- **Pause Handling:** Evaluates whether models correctly identify natural pauses without interrupting the speaker.
- **Backchanneling:** Measures how well models generate short acknowledgments at appropriate times.
- **Smooth Turn-Taking:** Assesses response timing to ensure fluid dialogue transitions.
- **User Interruption Management:** Tests a model's ability to adapt to user interruptions and shift focus appropriately.

We will provide a set of curated datasets, automatic evaluation scripts, and baseline results on multiple state-of-the-art full-duplex models in upcoming releases. Our benchmark aims to drive progress in spoken dialogue systems by encouraging fair and open evaluation practices. 

The audio demo samples can be found in [[Demo]](https://full-duplex-bench.github.io/).

## Timeline ‚è±
- **Dataset Release:** Coming soon ‚è≥
- **Codebase and Evaluation Framework:** Coming soon ‚è≥
- **Expanded Model Evaluations & Community Contributions:** Ongoing üöÄ

Stay tuned for upcoming releases!

## Getting Started üèÅ
### Installation
The repository and installation instructions will be available upon release.

### Running Evaluations
Evaluation scripts and datasets will be made available in the upcoming release.

## Citation üìñ
If you have any questions, please feel free to submit an issue or contact Guan-Ting Lin (daniel094144@gmail.com)

If you found this research helpful, please consider citing our work:

```
@misc{lin2025fullduplexbenchbenchmarkevaluatefullduplex,
      title={Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities}, 
      author={Guan-Ting Lin and Jiachen Lian and Tingle Li and Qirui Wang and Gopala Anumanchipalli and Alexander H. Liu and Hung-yi Lee},
      year={2025},
      eprint={2503.04721},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.04721}, 
}
```

